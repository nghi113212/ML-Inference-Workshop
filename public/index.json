[
{
	"uri": "//localhost:1313/6-ecr/6.1-create-ecr/",
	"title": "Create ECR",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the Elastic Container Registry service Select Elastic Container Registry from the search results In Amazon Elastic Container Registry\nIn Private registry, choose Repositories Click Create repository In General settings\nRepository name: Enter my-workshop-ecr Click Create Confirm\nSuccessful "
},
{
	"uri": "//localhost:1313/5-elastic-cache/5.1-create-serverless-cache/",
	"title": "Create serverless cache",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the ElastiCache service Select ElastiCache from the search results In Amazon ElastiCache\nSelect Valkey caches in the left-hand side Click Create cache In Configuration\nEngine: select Valkey Deployment option: choose Serverless Creation method: choose New cache In Settings\nFor Name: enter my-workshop-cache Engine version: select 8 Expand Default settings\nSelect Customize default settings In Connectivity\nVPC ID: Select my-workshop-vpc Availability Zones: Select ap-southeast-1a and ap-southeast-1b For Security\nChoose Customize your security settings\nIn Selected security groups, choose Manage In Manage security groups\nSelect valkey-sg Then, click Choose Scroll down, then click Create Wait for the Status turns into Available\nCopy the Endpoint and ARN, we will use this later "
},
{
	"uri": "//localhost:1313/4-vpc/4.1-create-vpc/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the VPC service Select VPC from the search results In the VPC Dashboard\nSelect Your VPCs from the left menu Click Create VPC Configure your VPC\nResources: Select VPC only Name tag: Enter my-workshop-vpc IPv4 CIDR: Enter 10.0.0.0/16 Tendency: Keep default Confirm VPC creation\nClick Create VPC Check after create\nSuccessful "
},
{
	"uri": "//localhost:1313/3-build-model/3.1-basic-sentiment-model/",
	"title": "Simple sentiment model",
	"tags": [],
	"description": "",
	"content": "A sentiment model is a type of machine learning model that can analyze a piece of text and determine the sentiment behind it—whether it\u0026rsquo;s positive, negative, or neutral. These models are commonly used in areas like: Customer feedback analysis, Social media monitoring, Product review classification,\u0026hellip;\nIn this section, we will walk through how to collect data, preprocess text, train a machine learning model, and make predictions using Python. By the end of this session, you\u0026rsquo;ll have a working sentiment analysis tool that you can experiment with or even improve further.\nLet\u0026rsquo;s get started Create environment for developing model Create a python virtual environment Create a new folder and open it with VS Code then type python -m venv \u0026lt;name of your virtual env\u0026gt; in your terminal. Create file create_sentiment_model.py Collect data In this workshop, instead of collecting raw data manually, we will use a pre-existing dataset for sentiment analysis. For this tutorial, we will use the IMDb movie review and Yelp data reviewer dataset. First, we will activate the virtual environment that was created before, typing \u0026lt;name of your venv\u0026gt;/Scripts/activate. Next, install dataset library to use ready-made dataset. pip install datasets Now, let see what inside these datasets. (You do not have to follow this step. This step just aims to let you have a specific look of this library) Building time Prepare train data and test data (Because the data in each dataset is quite small so the author decided to combine them together. If your prepared data is big enough, you do not hava to do this) Data preprocessing\nInstall scikit-learn library for create model pip install scikit-learn Purpose of this step: To convert text data into numerical format using TF-IDF (Term Frequency - Inverse Document Frequency) so that the machine learning model can understand and process the input. This step helps represent each document as a vector based on the importance of words, making it suitable for training and prediction.\nCreate model Evaluate model Save model and vectorizer Source code:\nfrom datasets import load_dataset\r# Load the datasets imdb_dataset = load_dataset(\u0026#34;imdb\u0026#34;)\ryelp_dataset = load_dataset(\u0026#34;yelp_polarity\u0026#34;)\r# Trích xuất dữ liệu train IMDB\rX_imdb_train = imdb_dataset[\u0026#39;train\u0026#39;][\u0026#39;text\u0026#39;][:]\ry_imdb_train = imdb_dataset[\u0026#39;train\u0026#39;][\u0026#39;label\u0026#39;][:]\r# Trích xuất dữ liệu train Yelp\rX_yelp_train = yelp_dataset[\u0026#39;train\u0026#39;][\u0026#39;text\u0026#39;][:]\ry_yelp_train = yelp_dataset[\u0026#39;train\u0026#39;][\u0026#39;label\u0026#39;][:]\r# Combine train datasets\rX_train = X_imdb_train + X_yelp_train\ry_train = y_imdb_train + y_yelp_train\r# Trích xuất dữ liệu test IMDB\rX_imdb_test = imdb_dataset[\u0026#39;test\u0026#39;][\u0026#39;text\u0026#39;][:]\ry_imdb_test = imdb_dataset[\u0026#39;test\u0026#39;][\u0026#39;label\u0026#39;][:]\r# Trích xuất dữ liệu test Yelp\rX_yelp_test = yelp_dataset[\u0026#39;test\u0026#39;][\u0026#39;text\u0026#39;][:]\ry_yelp_test = yelp_dataset[\u0026#39;test\u0026#39;][\u0026#39;label\u0026#39;][:]\r# Combine test datasets\rX_test = X_imdb_test + X_yelp_test\ry_test = y_imdb_test + y_yelp_test\r# Tiền xử lý dữ liệu\rfrom sklearn.feature_extraction.text import TfidfVectorizer\rvectorizer = TfidfVectorizer()\rX_train_tfidf = vectorizer.fit_transform(X_train)\rX_test_tfidf = vectorizer.transform(X_test)\r# Tạo mô hình\rfrom sklearn.linear_model import LogisticRegression\rmodel = LogisticRegression()\rmodel.fit(X_train_tfidf, y_train)\r# Đánh giá mô hình\rfrom sklearn.metrics import accuracy_score\ry_pred = model.predict(X_test_tfidf)\raccuracy = accuracy_score(y_test, y_pred)\rprint(f\u0026#34;Accuracy: {accuracy:.4f}\u0026#34;)\r# Lưu model\rimport pickle\rwith open(\u0026#39;sentiment_model.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f:\rpickle.dump(model, f)\r# Save vectorizer\rwith open(\u0026#39;tfidf_vectorizer.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f:\rpickle.dump(vectorizer, f) Train the model\nRun python create_sentiment_model.py This step will take time due to the strenght of your CPU since scikit-learn don\u0026rsquo;t run in GPU. Result: You will see the accuracy after training Two files .pkl were created Congratulation! You have successfully built a sentiment model with accuracy of 90,7%. Now let\u0026rsquo;s try to optimize it to have a better accuracy.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-dowload-vs-code/",
	"title": "Install Visual Studio Code",
	"tags": [],
	"description": "",
	"content": "Overview Choose an IDE that supports plug-ins for various languages to make development more convenient, such as Visual Studio Code, Atom, Notepad++, etc.\nPlug-ins by IDE: Naturally, you will need plug-ins for Markdown (such as Markdown All in One, Markdown TOC, etc.). When working with any language, VS Code will usually suggest the relevant plug-ins, so there\u0026rsquo;s no need to worry.\nInstructions Visit Visual Studio Code to download the IDE. Visit the Documentation page for detailed information and guidance on using extensions. "
},
{
	"uri": "//localhost:1313/",
	"title": "First Cloud Journey",
	"tags": [],
	"description": "",
	"content": "Real-time Machine Learning Inference with AWS Lambda \u0026amp; API Gateway Workshop Overview This workshop provides hands-on experience in building and deploying a production-ready machine learning inference system using AWS serverless technologies. You\u0026rsquo;ll learn to create a scalable ML API that delivers predictions with sub-100ms latency while maintaining cost efficiency and enterprise-level security.\nWhat You\u0026rsquo;ll Build A complete serverless ML inference solution featuring:\nUltra-fast API responses (\u0026lt; 100ms) using AWS Lambda and API Gateway Intelligent caching with ElastiCache Valkey for optimal performance Production monitoring using CloudWatch and AWS X-Ray Enterprise security with IAM roles and VPC integration Auto-scaling capabilities that handle traffic spikes seamlessly Key Technologies AWS Lambda - Serverless compute for ML model hosting API Gateway - RESTful API management and routing ElastiCache Valkey - High-performance caching layer Python 3.12 - Runtime environment with Scikit-learn CloudWatch \u0026amp; X-Ray - Comprehensive monitoring and tracing Learning Goals By completing this workshop, you will:\nKnow how to build a machine learning model Deploy production-ready ML models on AWS serverless infrastructure Implement effective caching strategies for ML workloads Set up comprehensive monitoring and alerting systems Apply AWS security best practices for enterprise applications Optimize system performance for real-time inference requirements Prerequisites Basic AWS console familiarity Understanding of REST APIs and HTTP concepts Python programming knowledge (intermediate level) AWS account with appropriate permissions Content 1. Introduction 2. Preparation 3. Build model 4. VPC 5. ElastiCache 6. ECR 7. Lambda 8. API Gateway 9. Testing 10. Monitoring and Pricing 11. Clean up "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Real-World Scenario Imagine you\u0026rsquo;re a data scientist at an e-commerce company. Your team has built a fraud detection model that needs to analyze every transaction in real-time. The business requirements are demanding:\nPerformance: Detect fraud within 50ms to prevent transaction delays Scale: Handle Black Friday traffic spikes from 100 to 10,000 requests per second Cost: Keep infrastructure costs under control during low-traffic periods Reliability: 99.9% uptime requirement for production systems Your current server-based deployment can\u0026rsquo;t meet these demands cost-effectively.\nThe Challenge Deploying machine learning models to production is complex and expensive. Traditional approaches struggle with:\nHigh latency - Business applications need sub-100ms response times Unpredictable scaling - Traffic spikes are hard to handle efficiently Infrastructure costs - Over-provisioned servers waste money Operational overhead - Managing servers, scaling, and monitoring Our Solution: Serverless ML Architecture AWS serverless architecture solves these problems by providing:\nPay-per-request pricing - No idle costs Automatic scaling - From zero to thousands of requests Sub-100ms latency - With provisioned concurrency Built-in monitoring - CloudWatch and X-Ray integration Enterprise security - IAM and VPC support Solution Architecture How it works:\nAPI Gateway receives and validates requests Lambda loads model and processes inference (with caching check) ElastiCache stores frequent predictions for instant retrieval CloudWatch monitors performance and logs all activities Learning Outcomes After completing this workshop, you will:\nKnow how to build a machine learning model Deploy scalable ML models on AWS serverless infrastructure Implement effective caching strategies for ML workloads Set up production monitoring and alerting Achieve consistent sub-100ms response times "
},
{
	"uri": "//localhost:1313/5-elastic-cache/5.2-create-cache-user/",
	"title": "Create ElastiCache user",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the ElastiCache service Select ElastiCache from the search results In Amazon ElastiCache\nSelect User management in the left-hand side Click Create user In User settings\nFor User ID and User name: Enter iam-user-01 Engine: Select Valkey In User authentication settings\nChoose IAM authentication In Access string\nEnter: on ~* +@all Then, click Create After created, copy ARN value of iam-user-01 we will use this later In Amazon ElastiCache\nChoose User group management Click Create user group In User group settings\nUser group ID: Enter iam-user-group-01 For Selected users, click Manage Select iam-user-01, then click Choose Scroll down, click Create Back to Valkey cache\nSelect my-workshop-cache In my-workshop-cache\nClick Modify Scroll down to Security\nAccess control: Select User group access control list User group: Select iam-user-group-01 Scroll down and click Preview changes Click Save changes Confirm\nSuccessfull "
},
{
	"uri": "//localhost:1313/4-vpc/4.2-create-private-subnet/",
	"title": "Create Private Subnet",
	"tags": [],
	"description": "",
	"content": " In VPC dashboard\nSelect Subnets from the left-hand menu Click Create subnet Select a VPC\nIn the Create subnet Choose the my-workshop-vpc created earlier Create the first private subnet\nSubnet name: Enter Private Subnet 1 Availability Zone: Choose ap-southeast-1a IPv4 subnet CIDR block: Enter 10.0.0.0/24 Click Create subnet Confirm\nSuccessful Create the second private subnet\nChoose the my-workshop-vpc created earlier Subnet name: Enter Private Subnet 2 Availability Zone: Choose ap-southeast-1b IPv4 subnet CIDR block: Enter 10.0.1.0/24 Click Create subnet Confirm\nSuccessful "
},
{
	"uri": "//localhost:1313/3-build-model/3.2-optimize--test-model/",
	"title": "Optimize and test model",
	"tags": [],
	"description": "",
	"content": "Optimization Parameter tuning/optimization for vectorizer\nIn file create_sentiment_model.py add to TfidfVectorizer() some parameters shown as follow: vectorizer = TfidfVectorizer(\rstop_words=\u0026#39;english\u0026#39;, ngram_range=(1,2),\rmax_features=80000,\rmax_df=0.9, min_df=2 ) Meaning:\nstop_words=\u0026lsquo;english\u0026rsquo; - Remove English stop words (the, and, is, in, \u0026hellip;) ngram_range=(1,2) - Use both unigrams (single words) and bigrams (2-word phrases) max_features=80000 - Limit to keep only the 80,000 most important features max_df=0.9 - Remove words that appear in \u0026gt;90% of documents (too common) min_df=2 - Remove words that appear in \u0026lt;2 documents (too rare) Optimizing LogisticRegression model parameters\nIn file create_sentiment_model.py add to LogisticRegression() some parameters shown as follow: model = LogisticRegression(\rmax_iter=250,\rC=2.0, solver=\u0026#39;saga\u0026#39;\r) Meaning:\nmax_iter=250: Maximum number of iterations for algorithm convergence (default is usually 100) - Increased to ensure model has enough time to learn all data C=2.0: Regularization parameter - Allows model to \u0026ldquo;learn\u0026rdquo; more details from the data solver=\u0026lsquo;saga\u0026rsquo;: Optimization algorithm - Good support for L1, L2 and Elastic Net regularization Pre-train the model to see the new accuracy\nRun python create_sentiment_model.py This will take time just wait! Result: Congratulation! You have successfully optimize model from 90,7% to 91,3% of accuracy. You can change value of those parameters above or add some parameters yourself to see the differences\nTest the model Create file test.py in the same directory\nUsing this code to test that model can predict a sentence if it is negative or positive\nimport pickle\r# Load model và vectorizer\rwith open(\u0026#39;sentiment_model.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f:\rmodel = pickle.load(f)\rwith open(\u0026#39;tfidf_vectorizer.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f:\rvectorizer = pickle.load(f)\r# Hàm dự đoán sentiment\rdef predict_sentiment(text):\rvec = vectorizer.transform([text])\rpred = model.predict(vec)[0]\rreturn \u0026#34;Positive\u0026#34; if pred == 1 else \u0026#34;Negative\u0026#34;\r# Test\rprint(predict_sentiment(\u0026#34;I love this movie, it was fantastic!\u0026#34;)) Run python test.py Let\u0026rsquo;s try a negative sentence So we have completed the first step of our topic is to have a inference model. Now, let\u0026rsquo;s move to the next step of our project.\n"
},
{
	"uri": "//localhost:1313/6-ecr/6.2-push-ml-image/",
	"title": "Push ML image to ECR",
	"tags": [],
	"description": "",
	"content": " Back to VS Code Editor\nCreate a new folder name my-workshop-ecr Move two .pkl files into this folder In my-workshop-ecr folder\nCreate file lambda_function.py with the content as given: import pickle\rimport json\rimport botocore.session\rfrom botocore.model import ServiceId\rfrom botocore.signers import RequestSigner\rfrom urllib.parse import urlencode, urlunparse, ParseResult\rimport redis\r# ==== Load model và vectorizer (chỉ chạy 1 lần khi cold start) ====\rwith open(\u0026#39;sentiment_model.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f:\rmodel = pickle.load(f)\rwith open(\u0026#39;tfidf_vectorizer.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as f:\rvectorizer = pickle.load(f)\r# ==== Generate IAM auth token (function) ====\rdef generate_iam_token(username, cache_name, region=\u0026#34;us-east-1\u0026#34;):\rsession = botocore.session.get_session()\rrequest_signer = RequestSigner(\rServiceId(\u0026#34;elasticache\u0026#34;),\rregion,\r\u0026#34;elasticache\u0026#34;,\r\u0026#34;v4\u0026#34;,\rsession.get_credentials(),\rsession.get_component(\u0026#34;event_emitter\u0026#34;),\r)\rquery_params = {\u0026#34;Action\u0026#34;: \u0026#34;connect\u0026#34;, \u0026#34;User\u0026#34;: username, \u0026#34;ResourceType\u0026#34;: \u0026#34;ServerlessCache\u0026#34;}\rurl = urlunparse(\rParseResult(\rscheme=\u0026#34;https\u0026#34;,\rnetloc=cache_name,\rpath=\u0026#34;/\u0026#34;,\rquery=urlencode(query_params),\rparams=\u0026#34;\u0026#34;,\rfragment=\u0026#34;\u0026#34;,\r)\r)\rsigned_url = request_signer.generate_presigned_url(\r{\u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;url\u0026#34;: url, \u0026#34;body\u0026#34;: {}, \u0026#34;headers\u0026#34;: {}, \u0026#34;context\u0026#34;: {}},\roperation_name=\u0026#34;connect\u0026#34;,\rexpires_in=900,\rregion_name=region,\r)\rreturn signed_url.removeprefix(\u0026#34;https://\u0026#34;)\r# ==== Setup Redis client (IAM auth) ====\rusername = \u0026#34;iam-user-01\u0026#34; # Thay bằng IAM user của bạn\rcache_name = \u0026#34;my-workshop-cache\u0026#34; # Thay bằng cache name\relasticache_endpoint = \u0026#34;my-workshop-cache-e8trvn.serverless.apse1.cache.amazonaws.com\u0026#34; # Endpoint cache thực tế\rregion = \u0026#34;ap-southeast-1\u0026#34; #Your cache region\rauth_token = generate_iam_token(username, cache_name, region)\rredis_client = redis.Redis(\rhost=elasticache_endpoint,\rport=6379,\rusername=username,\rpassword=auth_token,\rssl=True,\rssl_cert_reqs=\u0026#34;none\u0026#34;\r)\r# ==== Lambda handler ====\rdef lambda_handler(event, context):\rtry:\r# Parse input\rif \u0026#39;body\u0026#39; in event:\rbody = json.loads(event[\u0026#39;body\u0026#39;])\relse:\rbody = event\rtext = body.get(\u0026#39;text\u0026#39;)\rif not text:\rreturn {\r\u0026#39;statusCode\u0026#39;: 400,\r\u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Missing \u0026#34;text\u0026#34; in request\u0026#39;})\r}\r# Check cache first\rcached_value = redis_client.get(text)\rif cached_value:\rprediction, sentiment = json.loads(cached_value)\relse:\r# Predict if not cached\rX_input = vectorizer.transform([text])\rprediction = model.predict(X_input)[0]\rsentiment = \u0026#34;Positive\u0026#34; if prediction == 1 else \u0026#34;Negative\u0026#34;\r# Save to cache\rredis_client.setex(text, 3600, json.dumps((int(prediction), sentiment)))\r# Return result\rreturn {\r\u0026#39;statusCode\u0026#39;: 200,\r\u0026#39;body\u0026#39;: json.dumps({\r\u0026#39;text\u0026#39;: text,\r\u0026#39;prediction\u0026#39;: int(prediction),\r\u0026#39;sentiment\u0026#39;: sentiment\r})\r}\rexcept Exception as e:\rreturn {\r\u0026#39;statusCode\u0026#39;: 500,\r\u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)})\r} Replace some parameter with your own Info Watch out: Remember to remove the port number at the end of Cache Endpoint In my-workshop-ecr folder\nCreate Dockerfile With the content as given: FROM public.ecr.aws/lambda/python:3.12\r# Copy code\rCOPY lambda_function.py ${LAMBDA_TASK_ROOT}\rCOPY sentiment_model.pkl ${LAMBDA_TASK_ROOT}\rCOPY tfidf_vectorizer.pkl ${LAMBDA_TASK_ROOT}\r# Install requirements\rRUN pip install scikit-learn --target ${LAMBDA_TASK_ROOT}\rRUN pip install redis --target ${LAMBDA_TASK_ROOT}\rRUN pip install botocore --target ${LAMBDA_TASK_ROOT}\rRUN pip install cachetools --target ${LAMBDA_TASK_ROOT}\r# Command to run Lambda\rCMD [\u0026#34;lambda_function.lambda_handler\u0026#34;] Now we will use AWS CLI to handle this\nIf you don\u0026rsquo;t have AWS CLI you can get back to chapter 2 section 2.4 for installation. Make sure your account has Access keys You can create Access keys by following these steps: Access IAM Service, select User, choose Create access key In VS Code Editor\nIn terminal, type: cd my-workshop-ecr Next, type: aws configure Type your Access key Back to Elastic Container Registry\nClick my-workshop-ecr Click View push command Start your docker engine\nCopy the first command and type it in terminal Copy the second command and type it in terminal. Make sure your are in the my-workshop-ecr directory Type the third command in your terminal Type the fourth command in terminal After finish, go back to AWS ECR you will see your images "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-dowload-postman/",
	"title": "Install Postman",
	"tags": [],
	"description": "",
	"content": "Overview Postman is an essential tool for testing and developing APIs. In this ML Inference workshop, you\u0026rsquo;ll use Postman to:\nTest ML API endpoints deployed on AWS API Gateway Send inference requests with sample data to your serverless ML model Verify response times to ensure sub-100ms latency requirements Test authentication and security configurations Monitor API behavior under different load conditions Why Postman for ML APIs? Quick Testing: Send POST requests with JSON payloads containing features for prediction Response Analysis: View model predictions and response metadata Performance Monitoring: Check response times and debug latency issues Collection Management: Save ML inference requests for different use cases Environment Variables: Switch between development, staging, and production endpoints Instructions Visit the official Postman download page:\n👉 https://www.postman.com/downloads/\nChoose the correct version for your operating system:\nWindows: .exe installer macOS: .zip or .dmg file Linux: .tar.gz or AppImage Install and launch Postman. You can sign in or click \u0026ldquo;Skip and take me to the app\u0026rdquo;.\nKey Features for ML API Testing:\nRequest URL: Enter your API Gateway endpoint Method: Use POST for ML inference requests Headers: Set Content-Type: application/json Body: Include JSON with features for model prediction Tests: Add scripts to validate response format and latency Create Collections for ML Testing:\nML Inference - Fraud Detection Performance Tests Error Handling Tests Cache Validation Tests "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "Overview Before starting this ML Inference Workshop, you need to install and configure the following tools. This workshop focuses on deploying machine learning models as serverless containers using AWS Lambda and API Gateway for high-performance inference.\nRequired Tools 🛠️ Development Environment Visual Studio Code - IDE for code development with ML and Docker extensions Python - Programming language for ML model development and AWS integration 🐳 Containerization Docker Desktop - Package ML models into optimized containers for AWS Lambda deployment ☁️ AWS Integration AWS CLI - Command-line tool to push Docker images to Elastic Container Registry (ECR) 🧪 API Testing Postman - Test ML inference endpoints, verify sub-100ms response times, and validate API behavior Workshop Flow With these tools, you\u0026rsquo;ll be able to:\nDevelop ML models locally using Python Containerize models with Docker for Lambda deployment Push container images to AWS ECR using AWS CLI Deploy serverless inference APIs on AWS Lambda Test API performance and functionality with Postman Estimated Setup Time: 30-45 minutes for all tools installation and configuration\n"
},
{
	"uri": "//localhost:1313/3-build-model/",
	"title": "Build model",
	"tags": [],
	"description": "",
	"content": "Overview Building a successful machine learning model follows a systematic process with these key steps:\nStep 1: Define the Problem Identify problem type (classification, regression, clustering), establish success metrics (accuracy, precision, recall), and determine features needed for prediction.\nStep 2: Data Collection Build training dataset with high-quality labeled data, ensure sufficient volume, label accuracy, and representative sampling for the problem domain.\nStep 3: Data Analysis (EDA) Perform statistical analysis, find correlations between features and target variables, assess data quality and class balance to understand dataset characteristics.\nStep 4: Preprocessing and Feature Engineering Data cleaning (handle missing values, outliers), feature transformation (normalize, encode), feature selection and creation to prepare data for algorithms.\nStep 5: Algorithm Selection Choose appropriate learning algorithms based on problem requirements. For beginners: Decision Trees, Linear/Logistic Regression, k-NN.\nStep 6: Model Training Data splitting (80% train, 10% validation, 10% test), model fitting, hyperparameter optimization, and cross-validation to ensure robust performance.\nStep 7: Model Evaluation Measure performance metrics, generalization testing on test data, error analysis, and statistical significance testing to assess model effectiveness.\nStep 8: Improvement and Optimization Feature engineering refinement, algorithm comparison, ensemble methods, and error pattern analysis to optimize model performance systematically.\n"
},
{
	"uri": "//localhost:1313/5-elastic-cache/5.3-create-iam-role/",
	"title": "Create IAM role",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the IAM service Select IAM from the search results In Identity and Access Management (IAM)\nSelect Policies Click Create policy In Policy editor\nSelect JSON Replace cache ARN with your own ARN {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34; : [\r\u0026#34;elasticache:Connect\u0026#34;\r],\r\u0026#34;Resource\u0026#34; : [\r\u0026#34;arn:aws:elasticache:ap-southeast-1:878585013121:serverlesscache:my-workshop-cache\u0026#34;,\r\u0026#34;arn:aws:elasticache:ap-southeast-1:878585013121:user:iam-user-01\u0026#34;\r]\r}\r]\r} Scroll down, click Next In Policy Details\nFor Policy name: Enter elasticache-allow-all Scroll down, click Create policy Confirm\nSuccessful In Identity and Access Management (IAM)\nSelect Roles Click Create role For Trusted entity type\nChoose Custom trust policy {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Principal\u0026#34;: {\r\u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34;\r},\r\u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;\r}]\r} Click Next In Permissions policies\nFilter by Type Choose Customer managed Then click Next In Role details\nRole name: Enter elasticache-iam-auth-app Scroll down, click Create role Confirm\nSuccessfull "
},
{
	"uri": "//localhost:1313/4-vpc/4.3-create-route-table/",
	"title": "Create Route Table",
	"tags": [],
	"description": "",
	"content": "Create route table In VPC dashboard\nChoose Route tables frpm the left-hand sidebar Click on Create route table In Route table settings\nName: Route table-Private-1 VPC: Choose my-workshop-vpc Click Create route table Confirm creation\nSuccessful Repeat the same step for private route table 2\nName: Route table-Private-2 VPC: Choose my-workshop-vpc Click Create route table Confirm creation\nSuccessful Associate with subnet In Route tables dashboard\nSelect Route table-Private-1 Next, choose Subnet associations tab In Explicit subnet associations, click Edit subnet associations In Edit subnet associations\nSelect Private Subnet 1 Then, click Save associations Do the same for Route table-Private-2\nSelect Route table-Private-2 "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-dowload-docker/",
	"title": "Install Docker Desktop",
	"tags": [],
	"description": "",
	"content": "Overview Docker Desktop is essential for this ML Inference workshop because AWS Lambda supports container-based deployments. You\u0026rsquo;ll use Docker to:\nPackage ML models with their dependencies into containers Test locally before deploying to AWS Lambda Build optimized images for faster cold starts Manage Python dependencies and ML libraries efficiently Ensure consistent environments between development and production Why Containers for ML Inference? Larger Package Sizes: ML libraries like scikit-learn, numpy exceed Lambda\u0026rsquo;s 250MB limit Dependency Management: Complex ML dependencies are easier to manage in containers Performance Optimization: Pre-built images reduce cold start times Reproducible Deployments: Same environment across dev, staging, and production Installation Instructions 1. Download Docker Desktop Visit the official Docker website:\n👉 https://www.docker.com/products/docker-desktop/\nChoose your operating system:\nWindows: Docker Desktop for Windows (requires WSL2) macOS: Docker Desktop for Mac (Intel or Apple Silicon) Linux: Docker Desktop for Linux 2. System Requirements Windows:\nWindows 10/11 64-bit (Pro, Enterprise, Education) WSL2 feature enabled 4GB RAM minimum macOS:\nmacOS 10.15 or newer 4GB RAM minimum Linux:\n64-bit kernel and CPU virtualization support 4GB RAM minimum 3. Installation Process Download the installer from the official website Run the installer with administrator privileges Restart your computer when prompted Launch Docker Desktop and complete initial setup 4. Verify Installation Open terminal/command prompt and run:\ndocker --version docker run hello-world You should see Docker version information and a successful test run.\n"
},
{
	"uri": "//localhost:1313/4-vpc/4.4-ceate-security-gr/",
	"title": "Create Security Group",
	"tags": [],
	"description": "",
	"content": " In VPC dashboard\nSelect Security groups in the left-hand sidebar Click Create security group In Basic details\nFor Security group name: lambda-sg For Description: Security Group for Lambda functions accessing ElasticCache (Valkey) inside VPC VPC: Choose my-workshop-vpc For Tags Choose Add new tag Key: Enter Name Value: Enter lambda-sg Then, click Create security group Create a second security group for Valkey\nDo the same steps as above For Security group name: valkey-sg For Description: Allows inbound access from Lambda functions to ElasticCache (Valkey) VPC: Choose my-workshop-vpc For Inbound rules: Select Add rule Type: Select Custom TCP Port range: Enter 6379 Source: Choose Custom, select lambda-sg For Tags Choose Add new tag Key: Enter Name Value: Enter valkey-sg Then, click Create security group "
},
{
	"uri": "//localhost:1313/4-vpc/",
	"title": "VPC",
	"tags": [],
	"description": "",
	"content": "In this workshop, creating a Virtual Private Cloud (VPC) is an essential step to simulate a real-world network environment on AWS. Instead of relying on the default VPC, building a custom VPC allows you to fully control how services like Lambda, ElastiCache communicate with each other while enhancing the overall security and isolation of your resources. This also helps you gain hands-on experience with network segmentation, internal service connectivity, and security best practices that are commonly used in production systems.\nSpecifically, creating a VPC provides the following benefits:\nNetwork isolation: Separates public and private subnets to protect sensitive resources from unauthorized internet access.\nImproved security: Enables precise traffic control through security groups and route tables, keeping services like databases and caches private.\nReal-world networking practice: Allows Lambda functions inside private subnets to securely connect to internal services like ElastiCache without public exposure.\nProduction-like architecture: Mirrors how systems are deployed in real-world scenarios, making the workshop more relevant and practical.\nFoundation for more complex setups: Prepares you to implement advanced architectures using NAT Gateways, Load Balancers, and Service Mesh.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.4-dowload-aws-cli/",
	"title": "Install AWS CLI",
	"tags": [],
	"description": "",
	"content": "Overview AWS CLI (Command Line Interface) is the official command-line tool for managing AWS resources directly from the terminal. In this workshop, it will be used to connect to Elastic Container Registry to push model image from docker.\nInstallation Step 1: Download and Install Visit the official AWS CLI installation page: 👉 https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\nFor Windows:\nDownload the .msi installer and run it For macOS:\nbrew install awscli For Linux (Ubuntu/Debian):\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Step 2: Verify Installation aws --version Expected output: aws-cli/2.x.x Python/3.x.x\nConfiguration Create IAM User (Recommended) Security: Don\u0026rsquo;t use your root AWS account credentials. Create an IAM user instead.\nConfigure CLI aws configure Enter your credentials:\nAWS Access Key ID: [Your Access Key]\rAWS Secret Access Key: [Your Secret Key]\rDefault region name: us-east-1\rDefault output format: "
},
{
	"uri": "//localhost:1313/5-elastic-cache/",
	"title": "ElastiCache",
	"tags": [],
	"description": "",
	"content": "ElastiCache Valkey is used to dramatically improve ML inference performance by caching prediction results. Without caching, each ML inference request takes 300-500ms, but with caching we achieve 50ms response times (90% faster). For our fraud detection use case requiring sub-50ms responses at 10,000 requests/second during peak traffic, caching reduces Lambda compute costs by 70% while ensuring instant responses for repeated predictions. The system checks cache first, returns cached results immediately if found, or runs the ML model and caches the result for future requests, targeting 70%+ cache hit rate for optimal performance.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.5-dowload-python/",
	"title": "Install Python",
	"tags": [],
	"description": "",
	"content": "Overview Python is a programming language essential for machine learning model development and deployment. In this workshop, Python will be used to build ML models, create Docker containers, and integrate with AWS services.\nIn this workshop, authur uses Python version 3.12.4.\nInstallation Step 1: Download and Install Python Visit the official Python website: 👉 https://www.python.org/downloads/\nFor Windows:\nDownload Python 3.8+ installer (.exe) Check \u0026ldquo;Add Python to PATH\u0026rdquo; during installation Choose \u0026ldquo;Install for all users\u0026rdquo; For macOS:\n# Using Homebrew (recommended) brew install python # Or download from python.org For Linux (Ubuntu/Debian):\nsudo apt update sudo apt install python3 python3-pip python3-venv Step 2: Verify Installation python --version # or python3 --version Expected output: Python 3.8.x or higher\npip --version # or pip3 --version "
},
{
	"uri": "//localhost:1313/6-ecr/",
	"title": "ECR",
	"tags": [],
	"description": "",
	"content": "Amazon Elastic Container Registry (ECR) is a fully managed Docker container registry that makes it easy to store, manage, and deploy Docker container images. In this workshop, ECR serves as the central repository for our machine learning model container, which packages our sentiment analysis model with all its dependencies. ECR integrates seamlessly with AWS Lambda, allowing us to deploy containerized ML models that are too large or complex for traditional Lambda deployment packages. This approach provides better dependency management, faster cold start times for ML workloads, and easier versioning of our model deployments.\n"
},
{
	"uri": "//localhost:1313/7-lambda/",
	"title": "Lambda",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the Lambda service Select Lambda from the search results Choose Create a function\nIn Create function select Container image\nFunction name: Enter demo-ml-inference Container image URI choose Browser images Amazon ECR image repository select my-workshop-ecr Choose the existing image Then, click Select image Open IAM service in different tab\nChoose Role in the left-hand side Click into elasticache-iam-auth-app role Choose Add permissions Choose Attach policy Search AWSLambdaVPCAccessExecutionRole Choose AWSLambdaVPCAccessExecutionRole Click Add permissions Back to Lambda Tab, Expand Change default execution role\nChoose Use an existing role Existing role choose elasticache-iam-auth-app Expand Additional configurations\nVCP tick to Enable VPC select my-workshop-vpc Subnet select Private Subnet 1 and Private Subnet 2 Security group choose lambda-sg Scroll down, choose Create function Confirm\nSuccessful Test the lambda function\nChoose the Configuration Tab\nChoose Edit For Time out change into 1 min\nThen, choose Save Choose the Test Tab in your function For Event JSON, type:\n{\r\u0026#34;text\u0026#34;: \u0026#34;I love this movie\u0026#34;\r} Then choose Test Lambda function runs probably As you can see this Duration is 580.20 ms, this is call the Cold start. This happends when we first run the function or run it again after a long break.\nTo minimize this problem, we will configure our Lambda to run constantly by using Provisioned concurrency\nProvisioned concurrency Choose Configuration Tab\nIn the left-hand side, click Concurrency and recursion detection\nIf your Unreserved account concurrency looks like this That means:\nYou have other lambda functions in the account that is using the concurrency Your account is new (which is not your case as it is 4 month old) or your account is not having much usage, and aws has set lower limits for both cases. Here is short steps to solve this: Click Versions Tab\nChoose Publish new version For Version description enter v1 Click Publish After publish new version you will be already in Configuration Tab with Provisioned concurrency Choose Edit Enter the number of Provisioned concurrency that you want for your lambda function Click Save When you use Provisioned concurrency that means your function runs constantly and is always ready to use. However, the more provisioned concurrency you use the more money you pay\nWait for the status turn in to Ready "
},
{
	"uri": "//localhost:1313/8-api-gateway/",
	"title": "API Gateway",
	"tags": [],
	"description": "",
	"content": " Access the AWS Management Console\nSearch for the API Gateway service Select API Gateway from the search results In API Gateway dashboard\nClick APIs in the left-hand side Next, choose Create API in the right-hand side Scroll down to REST API\nClick Build In API details\nChoose New API For API Name enter my-workshop-api Then, click Create API After created\nClick Create resource In Resource details\nFor Resource name enter predict Click Create resource After created\nClick Create method In Method details\nFor Method type select POST\nFor Integration type choose Lambda function\nFor Lambda function select Your lambda function Scroll down and click Create method After created\nClick Deploy API In Deploy API, for Stage select New stage For Stage name enter prod Then click Deploy Confirm\nSuccessful We will use the Invoke URL to invoke our Lambda function\n"
},
{
	"uri": "//localhost:1313/9-testing/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "Test with Postman Open Postman If you don’t have Postman you can get back to chapter 2 section 2.2 for installation. Create your own workspace In Postman Click +, for creating new request Select POST Paste the Invoke URL next to it and add /predict at the end of the URL Choose Body Tab (under the URL) Next, choose raw Then type: {\r\u0026#34;text\u0026#34;: \u0026#34;This movie is awsome\u0026#34;\r} Click Send and view the result Test with Locust Back to VS Code Editor\nInstall Locust library pip install locust Create a new file name locust.py with the content as given: from locust import HttpUser, task, between\rimport json\rimport random\rclass SentimentLoadTest(HttpUser):\rwait_time = between(0, 1) # mỗi user gửi request sau 1-2s\r@task\rdef test_sentiment_api(self):\r# Data mẫu, chỉnh theo input model của bạn\rexample_texts = [\r\u0026#34;I love this product!\u0026#34;,\r\u0026#34;This is terrible, I hate it.\u0026#34;,\r\u0026#34;It\u0026#39;s okay, not too bad but not great.\u0026#34;,\r\u0026#34;Absolutely wonderful experience.\u0026#34;,\r\u0026#34;Worst purchase ever made.\u0026#34;,\r\u0026#34;I\u0026#39;m very happy with my purchase.\u0026#34;,\r\u0026#34;The service was horrible.\u0026#34;,\r\u0026#34;I will definitely buy this again.\u0026#34;,\r\u0026#34;I regret buying this.\u0026#34;,\r\u0026#34;This made my day so much better.\u0026#34;,\r\u0026#34;The quality is disappointing.\u0026#34;,\r\u0026#34;I feel amazing after using this.\u0026#34;,\r\u0026#34;Never buying from here again.\u0026#34;,\r\u0026#34;This is just perfect for my needs.\u0026#34;,\r\u0026#34;It doesn\u0026#39;t work as expected.\u0026#34;,\r\u0026#34;Highly recommend this to everyone.\u0026#34;,\r\u0026#34;Such a waste of money.\u0026#34;,\r\u0026#34;It\u0026#39;s fine, does the job.\u0026#34;,\r\u0026#34;Exceeded my expectations.\u0026#34;,\r\u0026#34;I want a refund immediately.\u0026#34;,\r\u0026#34;Fantastic experience overall.\u0026#34;,\r\u0026#34;Completely useless product.\u0026#34;,\r\u0026#34;Satisfied with what I got.\u0026#34;,\r\u0026#34;Terrible customer service.\u0026#34;,\r\u0026#34;Superb quality and fast delivery.\u0026#34;,\r\u0026#34;Not worth the price at all.\u0026#34;,\r\u0026#34;Works like a charm!\u0026#34;,\r\u0026#34;Disappointed with this purchase.\u0026#34;,\r\u0026#34;Everything was great.\u0026#34;,\r\u0026#34;The item arrived broken.\u0026#34;,\r\u0026#34;Loved it so much!\u0026#34;,\r\u0026#34;Poor packaging and bad quality.\u0026#34;,\r\u0026#34;I am so pleased with this.\u0026#34;,\r\u0026#34;Doesn\u0026#39;t match the description.\u0026#34;,\r\u0026#34;Exactly what I was looking for.\u0026#34;,\r\u0026#34;Extremely dissatisfied.\u0026#34;,\r\u0026#34;Couldn\u0026#39;t be happier.\u0026#34;,\r\u0026#34;I hate it so much.\u0026#34;,\r\u0026#34;This is awesome!\u0026#34;,\r\u0026#34;Worst experience ever.\u0026#34;,\r\u0026#34;Very good product for the price.\u0026#34;,\r\u0026#34;It broke after one use.\u0026#34;,\r\u0026#34;This is my favorite purchase.\u0026#34;,\r\u0026#34;Very low quality material.\u0026#34;,\r\u0026#34;Best thing I\u0026#39;ve bought this year.\u0026#34;,\r\u0026#34;Completely not what I expected.\u0026#34;,\r\u0026#34;I\u0026#39;m extremely happy with it.\u0026#34;,\r\u0026#34;Such an awful experience.\u0026#34;,\r\u0026#34;Five stars from me.\u0026#34;,\r\u0026#34;Would not recommend to anyone.\u0026#34;,\r\u0026#34;It works perfectly.\u0026#34;,\r\u0026#34;Terrible choice I made.\u0026#34;,\r\u0026#34;Beyond my expectations.\u0026#34;,\r\u0026#34;Do not waste your money.\u0026#34;,\r\u0026#34;This is the best!\u0026#34;,\r\u0026#34;I\u0026#39;m so mad about this.\u0026#34;,\r\u0026#34;Loved everything about it.\u0026#34;,\r\u0026#34;I wish I never bought this.\u0026#34;,\r\u0026#34;Absolutely loved it.\u0026#34;,\r\u0026#34;Broke within days.\u0026#34;,\r\u0026#34;I\u0026#39;m very satisfied.\u0026#34;,\r\u0026#34;Worst product on the market.\u0026#34;,\r\u0026#34;Super happy with my purchase.\u0026#34;,\r\u0026#34;I can\u0026#39;t stand this product.\u0026#34;,\r\u0026#34;Will buy again for sure.\u0026#34;,\r\u0026#34;Not as described at all.\u0026#34;,\r\u0026#34;Amazing quality and service.\u0026#34;,\r\u0026#34;Terribly disappointed.\u0026#34;,\r\u0026#34;Very impressed with this.\u0026#34;,\r\u0026#34;Save your money for something better.\u0026#34;,\r\u0026#34;This made me so happy.\u0026#34;,\r\u0026#34;It\u0026#39;s a complete disaster.\u0026#34;,\r\u0026#34;I really like this product.\u0026#34;,\r\u0026#34;I don\u0026#39;t recommend this.\u0026#34;,\r\u0026#34;It feels amazing to use.\u0026#34;,\r\u0026#34;Completely failed my expectations.\u0026#34;,\r\u0026#34;I would buy it again.\u0026#34;,\r\u0026#34;Horrible experience overall.\u0026#34;,\r\u0026#34;Works as advertised.\u0026#34;,\r\u0026#34;Do not buy this.\u0026#34;,\r\u0026#34;It makes me feel great.\u0026#34;,\r\u0026#34;I\u0026#39;m never using this again.\u0026#34;,\r\u0026#34;Best purchase I\u0026#39;ve made recently.\u0026#34;,\r\u0026#34;Worst customer experience ever.\u0026#34;,\r\u0026#34;It does what it says.\u0026#34;,\r\u0026#34;Absolutely not worth it.\u0026#34;,\r\u0026#34;I\u0026#39;m so glad I bought this.\u0026#34;,\r\u0026#34;This is the worst thing ever.\u0026#34;,\r\u0026#34;It\u0026#39;s exactly as shown.\u0026#34;,\r\u0026#34;Disgusting product.\u0026#34;,\r\u0026#34;Very happy with my order.\u0026#34;,\r\u0026#34;I am so disappointed.\u0026#34;,\r\u0026#34;This is fantastic.\u0026#34;,\r\u0026#34;Terrible, just terrible.\u0026#34;,\r\u0026#34;It\u0026#39;s brilliant.\u0026#34;,\r\u0026#34;Does not work at all.\u0026#34;,\r\u0026#34;Such great value for money.\u0026#34;,\r\u0026#34;Awful quality.\u0026#34;,\r\u0026#34;Perfect in every way.\u0026#34;,\r\u0026#34;Completely unusable.\u0026#34;\r]\rinput_text = random.choice(example_texts)\rpayload = {\r\u0026#34;text\u0026#34;: input_text # thay đổi key này nếu API của bạn cần key khác\r}\rheaders = {\r\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;\r}\rwith self.client.post(\u0026#34;/predict\u0026#34;, data=json.dumps(payload), headers=headers, catch_response=True) as response:\rif response.status_code == 200:\rresponse.success()\relse:\rresponse.failure(f\u0026#34;Failed with status code {response.status_code}, body: {response.text}\u0026#34;) Run locust -f locust.py --host \u0026lt;Your Invoke URL\u0026gt; Open your browser access http://localhost:8089/ For number of user enter the number you want For Ramp up enter 1 Click START Wait one minute then click STOP Result: Most satisfies the requirement (\u0026lt;100ms). However, with 99%ile is 130ms so it is not satisfy the requirement (\u0026lt;100ms) Click CHARTS Tab to see specifically "
},
{
	"uri": "//localhost:1313/10-monitoring/",
	"title": "Monitoring and Pricing",
	"tags": [],
	"description": "",
	"content": "Monitoring In your Lambda function dashboard\nClick Configuration Tab Choose Monitoring and operations tools in the left sidebar In Additional monitoring tools choose Edit Activate X-Ray Click Save Confirm\nX-Ray is enable Access CloudWatch service (Your invoke lambda function again)\nChoose trace Select Trace you want to see insight Cost analysis Monthly Cost Calculation\nService Configuration:\nECR: 1 image (358.18MB) ElastiCache Serverless: Valkey engine Lambda: 50 Provisioned Concurrency units CloudWatch: Log monitoring X-Ray: Distributed tracing API Gateway: REST API only Detailed Monthly Cost Breakdown\nECR (Elastic Container Registry)\nStorage: 358.18MB = 0.358GB\rPricing: $0.10 per GB per month\rMonthly Cost: 0.358 × $0.10 = $0.036 ElastiCache Serverless (Valkey)\nBased on usage pattern: ~2.9 GB-Hours daily\rMonthly estimate: 2.9 × 30 = 87 GB-Hours\rPricing: $0.101 per GB-Hour for data storage\rMonthly Cost: 87 × $0.101 = $8.79 Lambda with Provisioned Concurrency ⚠️ MAJOR COST\nConfiguration: 50 concurrent executions\rMemory allocation: 1GB (typical for ML workloads)\rMonthly compute time: 50 × 1GB × 30 days × 24 hours × 3600 seconds\r= 129,600,000 GB-seconds per month\rPricing: $0.0000041667 per GB-second\rMonthly Cost: 129,600,000 × $0.0000041667 = $540.00 API Gateway (REST API)\nEstimated monthly requests: ~30,000 (scaled from workshop usage)\rFree Tier: First 1 million requests per month\rMonthly Cost: $0.00 (within free tier) CloudWatch + X-Ray\nCloudWatch Logs: Basic monitoring within free tier\rX-Ray Traces: Estimated 200 traces/month (within 100K free tier)\rMonthly Cost: $0.00 (within free tier) Total Monthly Cost Summary\nService Monthly Cost Percentage Lambda (Provisioned Concurrency) $540.00 98.4% ElastiCache Serverless $8.79 1.6% ECR Storage $0.036 0.007% API Gateway $0.00 0% CloudWatch/X-Ray $0.00 0% TOTAL $548.83 100% Cost Optimization Strategies\nCritical Recommendation: The current setup with 50 Provisioned Concurrency is extremely expensive for production.\nAlternative Configurations:\nOption 1: Reduce Provisioned Concurrency\n5 Provisioned Concurrency units instead of 50\rMonthly Cost: $54.00 (90% savings)\rTotal Monthly: $62.83 Option 2: Business Hours Only (8AM-8PM)\n50 units × 12 hours/day instead of 24/7\rMonthly Cost: $270.00 (50% savings)\rTotal Monthly: $278.83 Option 3: No Provisioned Concurrency\nUse regular Lambda with cold starts\rMonthly Cost: ~$2-5 for compute + requests\rTotal Monthly: $10.83 (98% savings) Production Recommendations\nFor Development/Testing:\nUse Option 3 (No Provisioned Concurrency): $10.83/month For Production (Low Traffic):\nUse Option 1 (5 Provisioned Concurrency): $62.83/month For Production (High Performance):\nUse Option 2 (Business Hours Only): $278.83/month Critical Cost Alert: Your current configuration costs $548.83/month - this is primarily due to 50 Provisioned Concurrency units running 24/7. Consider reducing to 5-10 units or implementing time-based scheduling for significant savings.\nCost Optimization Impact:\nCurrent setup: $548.83/month Optimized for testing: $10.83/month (98% savings) Optimized for production: $62.83/month (89% savings) "
},
{
	"uri": "//localhost:1313/11-clean-up/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Delete Lambda function In Lambda function dashboard Choose demo-ml-inference Click Action select Delete Type confirm Click Delete Delete ElastiCache In ElastiCache dashboard Choose your Cache Click Action choose Delete For Create backup choose No Type your cache\u0026rsquo;s name Click Delete In User management delete iam-user-01 Click Delete In User group management delete iam-user-group-01 Delete Elastic Container Registry In Elastic Container Registry dashboard Choose your repository Click Delete Delete API Gateway In API Gateway dashboard Choose your api Click Delete Delete Policy and Role In Policies dashboard Delete elasticache-allow-all and AWSLambdaTracerAccessExecutionRole-... In Role dashboard delete elasticache-iam-auth-app "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]