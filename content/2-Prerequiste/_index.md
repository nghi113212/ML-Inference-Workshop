---
title : "Preparation "
date : "2024-07-27"
weight : 2
chapter : false
pre : " <b> 2. </b> "
---

## Overview

Before starting this **ML Inference Workshop**, you need to install and configure the following tools. This workshop focuses on deploying machine learning models as **serverless containers** using **AWS Lambda** and **API Gateway** for high-performance inference.

---

## Required Tools

### üõ†Ô∏è Development Environment
- **[Visual Studio Code](2.1-dowload-vs-code)** - IDE for code development with ML and Docker extensions
- **[Python](2.5-dowload-python)** - Programming language for ML model development and AWS integration

### üê≥ Containerization
- **[Docker Desktop](2.3-dowload-docker)** - Package ML models into optimized containers for AWS Lambda deployment

### ‚òÅÔ∏è AWS Integration  
- **[AWS CLI](2.4-dowload-aws-cli)** - Command-line tool to push Docker images to Elastic Container Registry (ECR)

### üß™ API Testing
- **[Postman](2.2-dowload-postman)** - Test ML inference endpoints, verify sub-100ms response times, and validate API behavior

---

## Workshop Flow

With these tools, you'll be able to:

1. **Develop** ML models locally using Python
2. **Containerize** models with Docker for Lambda deployment  
3. **Push** container images to AWS ECR using AWS CLI
4. **Deploy** serverless inference APIs on AWS Lambda
5. **Test** API performance and functionality with Postman

{{% notice info %}}
**Estimated Setup Time**: 30-45 minutes for all tools installation and configuration
{{% /notice %}}

